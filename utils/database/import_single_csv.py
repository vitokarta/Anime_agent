"""å–®ä¸€ CSV æª”æ¡ˆåŒ¯å…¥å·¥å…·ï¼ˆä¸­æ–‡ç°¡åŒ–ç‰ˆï¼‰

ç”¨é€”ï¼šä¸€æ¬¡åŒ¯å…¥ä¸€å€‹æª”æ¡ˆï¼Œé¿å…æ‰¹æ¬¡æµç¨‹éæ–¼è¤‡é›œã€‚

ä¾†æº CSV æ¬„ä½ï¼ˆéœ€è¦å­˜åœ¨ï¼‰ï¼š
"img-fit-cover src","entity_localized_name","anime_tag","anime_summary","anime_story",
"steam-site-name","steam-site-name 2","steam-site-name 3","anime_tag 2","anime_tag 3",
"scormem-item","scormem-item 2","image_path"

æ¬„ä½å°æ‡‰ï¼š
- title â† entity_localized_name
- season â† ç”±æª”åæ¨å¾—ï¼šYYYY_1â†’Winter, YYYY_4â†’Spring, YYYY_7â†’Summer, YYYY_10â†’Fall  => ä¾‹å¦‚ 2024_1_with_image.csv -> 2024-Winter
- rating â† scormem-item ï¼ˆç©ºå­—ä¸²â†’NULLï¼‰
- viewers_count â† scormem-item 2 ï¼ˆåŸæ¨£å­—ä¸² e.g. "487K"ï¼‰
- genres_json â† anime_tag / anime_tag 2 / anime_tag 3 åˆä½µå»é‡ JSON é™£åˆ—
- platforms_json â† steam-site-name 1/2/3 åˆä½µå»é‡ JSON é™£åˆ—
- synopsis â† anime_storyï¼ˆç©ºå­—ä¸²â†’NULLï¼‰
- image_path â† image_path
- is_disliked â† é è¨­ 0ï¼ˆè³‡æ–™è¡¨ defaultï¼‰
- created_at â† DB default

ä½¿ç”¨æ–¹å¼ï¼š
python utils/database/import_single_csv.py anime_data/data/2024_1_with_image.csv

é¸é …ï¼š
--replace åŒå­£åŒåè‹¥å·²å­˜åœ¨å‰‡è¦†è“‹ï¼ˆä»¥ title + season ç•¶å”¯ä¸€æ¢ä»¶ï¼‰

æœªä¾†å¯æ“´å……ï¼šæ–°å¢ viewers_numericï¼ˆæŠŠ 487K / 1.2M è½‰æˆæ•´æ•¸ï¼‰
"""
from __future__ import annotations
import csv
import json
import sqlite3
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# --- å‹•æ…‹åŒ¯å…¥ï¼ˆæ”¯æ´ç›´æ¥ python åŸ·è¡Œç„¡å¥—ä»¶èªå¢ƒï¼‰ ---
try:  # å˜—è©¦å¥—ä»¶å¼ç›¸å°åŒ¯å…¥
    from .create_schema import DB_PATH, create_schema  # type: ignore
except Exception:  # ç›´æ¥åŸ·è¡Œæ™‚æœƒå¤±æ•—ï¼šattempted relative import
    ROOT = Path(__file__).resolve().parents[2]  # å°ˆæ¡ˆæ ¹ç›®éŒ„
    if str(ROOT) not in sys.path:
        sys.path.insert(0, str(ROOT))
    from utils.database.create_schema import DB_PATH, create_schema  # type: ignore

SEASON_CODE_MAP = {"1": "Winter", "4": "Spring", "7": "Summer", "10": "Fall"}
REQUIRED_COLUMNS = [
    "img-fit-cover src",
    "entity_localized_name",
    "anime_tag",
    "anime_summary",
    "anime_story",
    "steam-site-name",
    "steam-site-name 2",
    "steam-site-name 3",
    "anime_tag 2",
    # "anime_tag 3" æ”¹ç‚ºå¯é¸
    "scormem-item",
    "scormem-item 2",
    "image_path",
]


def derive_season(csv_path: Path) -> Optional[str]:
    stem = csv_path.stem  # 2024_1_with_image
    parts = stem.split("_")
    if len(parts) < 2:
        return None
    year, code = parts[0], parts[1]
    if not year.isdigit():
        return None
    season_word = SEASON_CODE_MAP.get(code)
    if not season_word:
        return None
    return f"{year}-{season_word}"


def build_unique_list(values: List[str]) -> List[str]:
    seen = set()
    result = []
    for v in values:
        if v and v not in seen:
            seen.add(v)
            result.append(v)
    return result


def parse_csv_row(row: Dict[str, str]) -> Dict:
    title = (row.get("entity_localized_name") or "").strip()
    rating_raw = (row.get("scormem-item") or "").strip()
    rating = float(rating_raw) if rating_raw else None
    viewers_count = (row.get("scormem-item 2") or "").strip() or None

    tags = build_unique_list([
        (row.get("anime_tag") or "").strip(),
        (row.get("anime_tag 2") or "").strip(),
        (row.get("anime_tag 3") or "").strip(),  # å¯èƒ½ä¸å­˜åœ¨ï¼Œrow.get æœƒå›å‚³ None
    ])
    platforms = build_unique_list([
        (row.get("steam-site-name") or "").strip(),
        (row.get("steam-site-name 2") or "").strip(),
        (row.get("steam-site-name 3") or "").strip(),
    ])
    synopsis = (row.get("anime_story") or "").strip() or None
    image_path = (row.get("image_path") or "").strip() or None

    return {
        "title": title,
        "rating": rating,
        "viewers_count": viewers_count,
        "genres_json": json.dumps(tags, ensure_ascii=False),
        "platforms_json": json.dumps(platforms, ensure_ascii=False),
        "synopsis": synopsis,
        "image_path": image_path,
    }


def upsert_anime(cur: sqlite3.Cursor, season: str, record: Dict, replace: bool):
    """ä¾ç…§ title åˆ¤æ–·æ˜¯å¦å·²å­˜åœ¨ï¼ˆå¿½ç•¥ seasonï¼‰ã€‚å­˜åœ¨å°± skipï¼Œä¸åšæ›´æ–°ã€‚"""
    cur.execute("SELECT id FROM anime WHERE title = ?", (record["title"],))
    existing = cur.fetchone()
    if existing:
        return "skip"
    cur.execute(
        """
        INSERT INTO anime (title, season, rating, viewers_count, genres_json, platforms_json, image_path, synopsis)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """,
        (
            record["title"],
            season,
            record["rating"],
            record["viewers_count"],
            record["genres_json"],
            record["platforms_json"],
            record["image_path"],
            record["synopsis"],
        ),
    )
    return "insert"


def import_single(csv_file: Path, db_path: Path = DB_PATH, replace: bool = False) -> Tuple[int, int, int]:  # replace åƒæ•¸ä¿ç•™ä½†ä¸å†ä½¿ç”¨
    create_schema(db_path)
    season = derive_season(csv_file) or None
    with csv_file.open(encoding="utf-8-sig", newline="") as f:
        reader = csv.DictReader(f)
        missing = [c for c in REQUIRED_COLUMNS if c not in reader.fieldnames]
        if missing:
            raise ValueError(f"CSV ç¼ºå°‘æ¬„ä½: {missing}")
        rows = [parse_csv_row(r) for r in reader if (r.get("entity_localized_name") or "").strip()]

    conn = sqlite3.connect(db_path.as_posix())
    try:
        cur = conn.cursor()
        inserted = updated = skipped = 0
        for rec in rows:
            action = upsert_anime(cur, season, rec, replace=replace)
            if action == "insert":
                inserted += 1
            else:
                skipped += 1
        conn.commit()
        print(f"âœ… å®Œæˆ: æ–°å¢ {inserted} ç­†ï½œç•¥é {skipped} ç­†ï¼ˆä¾ title åˆ¤æ–·é‡è¤‡è·³éï¼‰ï½œSeason={season}")
        return inserted, updated, skipped
    finally:
        conn.close()


def auto_import_all(data_dir: Path = Path("anime_data/data"), db_path: Path = DB_PATH) -> None:
    """è‡ªå‹•åŒ¯å…¥ç›®éŒ„ä¸‹æ‰€æœ‰ *_with_image.csv æª”æ¡ˆï¼ˆæ’åºå¾Œï¼‰ï¼Œé‡è¤‡ title ç›´æ¥ç•¥éã€‚"""
    if not data_dir.exists():
        print(f"âŒ è³‡æ–™å¤¾ä¸å­˜åœ¨: {data_dir}")
        return
    csv_files = sorted(data_dir.glob("*_with_image.csv"))
    if not csv_files:
        print("âš ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½• *_with_image.csv æª”æ¡ˆ")
        return
    print("ğŸš€ é–‹å§‹è‡ªå‹•åŒ¯å…¥æ‰€æœ‰æª”æ¡ˆï¼ˆé‡è¤‡ title ç•¥éï¼‰...")
    total_insert = total_skip = 0
    for f in csv_files:
        print(f"--- åŒ¯å…¥ {f.name} ---")
        try:
            ins, _upd, skp = import_single(f, db_path=db_path, replace=False)
            total_insert += ins
            total_skip += skp
        except Exception as e:
            print(f"âŒ æª”æ¡ˆ {f.name} åŒ¯å…¥å¤±æ•—: {e}")
    print("ğŸ“Š åŒ¯å…¥ç¸½çµï¼š")
    print(f"  æ–°å¢: {total_insert}ï½œç•¥é(é‡è¤‡): {total_skip}")


if __name__ == "__main__":
    # è‹¥ä½¿ç”¨è€…æœ‰å¸¶åƒæ•¸ï¼Œä»æ”¯æ´èˆŠæ¨¡å¼ï¼›è‹¥ç„¡åƒæ•¸ â†’ è‡ªå‹•å…¨éƒ¨åŒ¯å…¥
    if len(sys.argv) == 1:
        auto_import_all()
    else:
        import argparse
        parser = argparse.ArgumentParser(description="å–®ä¸€å‹•ç•« CSV åŒ¯å…¥å·¥å…· / ç„¡åƒæ•¸ = è‡ªå‹•å…¨éƒ¨åŒ¯å…¥")
        parser.add_argument("csv", type=Path, nargs="?", help="CSV æª”æ¡ˆè·¯å¾‘ e.g. anime_data/data/2024_1_with_image.csv")
        parser.add_argument("--db", type=Path, default=DB_PATH, help="SQLite DB è·¯å¾‘")
        parser.add_argument("--replace", action="store_true", help="è‹¥åŒå­£åŒåå­˜åœ¨å‰‡è¦†è“‹ (é è¨­ Falseï¼›è‡ªå‹•æ¨¡å¼å›ºå®š True)")
        args = parser.parse_args()
        if args.csv:
            import_single(args.csv, args.db, replace=args.replace)
        else:
            auto_import_all(db_path=args.db)
